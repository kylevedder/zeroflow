#!/usr/bin/env python3
import sys

sys.path.insert(0, '.')

import argparse
from pathlib import Path
from typing import List,Optional
from loader_utils import run_cmd

parser = argparse.ArgumentParser()
parser.add_argument('command', type=str)
parser.add_argument('--job_dir', type=Path, default="./job_dir/")
parser.add_argument('--num_gpus', type=int, default=1)
parser.add_argument('--cpus_per_gpu', type=int, default=2)
parser.add_argument('--mem_per_gpu', type=int, default=12)
parser.add_argument('--runtime_mins', type=int, default=180)
parser.add_argument('--runtime_hours', type=int, default=None)
parser.add_argument('--job_name', type=str, default='ff3d')
parser.add_argument('--qos', type=str, default='ee-med')
parser.add_argument('--partition', type=str, default='eaton-compute')
parser.add_argument('--dependency', type=int, default=None)
parser.add_argument('--dry_run', action='store_true')
parser.add_argument('--blacklist_substring', type=str, default=None)
args = parser.parse_args()

num_prior_jobs = len(list(args.job_dir.glob("*")))
jobdir_path = args.job_dir / f"{num_prior_jobs:06d}"
jobdir_path.mkdir(exist_ok=True, parents=True)
job_runtime_mins = args.runtime_mins if args.runtime_hours is None else args.runtime_hours * 60


def load_available_nodes():
    res = run_cmd("sinfo --Node | awk '{print $1}' | tail +2",
                  return_stdout=True)
    available_nodes = res.split('\n')
    return [e.strip() for e in available_nodes]


def get_node_blacklist():
    if args.blacklist_substring is None:
        return []

    node_list = load_available_nodes()
    node_blacklist_list = []
    if "," in args.blacklist_substring:
        node_blacklist_list = [
            e.strip() for e in args.blacklist_substring.split(",")
        ]
    else:
        node_blacklist_list = [args.blacklist_substring]

    print(f"Blacklisting nodes with substrings {node_blacklist_list}")
    print(f"Available nodes: {node_list}")

    def is_blacklisted(node):
        for blacklist_str in node_blacklist_list:
            if blacklist_str in node:
                return True
        return False

    node_blacklist = [node for node in node_list if is_blacklisted(node)]
    # Deduplicate list
    node_blacklist = list(set(node_blacklist))
    print(f"Blacklisted nodes: {node_blacklist}")
    return node_blacklist


def get_runtime_format(runtime_mins):
    hours = runtime_mins // 60
    minutes = runtime_mins % 60
    return f"{hours:02d}:{minutes:02d}:00"


def make_command_file(command):
    command_path = jobdir_path / f"command.sh"
    command_file_content = f"""#!/bin/bash
{command}
"""
    with open(command_path, "w") as f:
        f.write(command_file_content)


def make_gpu_config():
    return f"""#SBATCH --gpus={args.num_gpus}
#SBATCH --mem-per-gpu={args.mem_per_gpu}G
#SBATCH --cpus-per-gpu={args.cpus_per_gpu}"""

def make_cpu_config():
    return f"""#SBATCH --mem-per-cpu={args.mem_per_gpu // args.cpus_per_gpu}G
#SBATCH --cpus-per-task={args.cpus_per_gpu}"""





def make_sbatch_file_content(current_working_dir: Path, docker_image_path: Path, is_gpu : bool):
    mount_dict = {
        "../../datasets/": "/efs/",
        current_working_dir:"/project",
        "/mnt/kostas-graid/":"/mnt/kostas-graid/",
        "/mnt/kostas-graid/datasets/argoverse_lidar_subsampled_all_hdd": "/efs/argoverse_lidar_subsampled_all",
        "/mnt/kostas-graid/datasets/argoverse_lidar_subsampled_all_complimentary": "/efs/argoverse_lidar_subsampled_all_complimentary",
        "/mnt/kostas-graid/datasets/argoverse2/val":"/efs/argoverse2/val",
        "/mnt/kostas-graid/datasets/argoverse2/val_sceneflow":"/efs/argoverse2/val_sceneflow",
        "/mnt/kostas-graid/datasets/argoverse2/train_sceneflow":"/efs/argoverse2/train_sceneflow",
        "/mnt/kostas-graid/datasets/argoverse2/train_nsfp_flow":"/efs/argoverse2/train_nsfp_flow",

    }

    return f"""#!/bin/bash
#SBATCH --job-name={args.job_name}
#SBATCH --qos={args.qos}
#SBATCH --partition={args.partition}
#SBATCH --nodes=1
#SBATCH --output={jobdir_path}/job.out
#SBATCH --error={jobdir_path}/job.err
#SBATCH --time={get_runtime_format(job_runtime_mins)}
#SBATCH --dependency={"afterok:" + str(args.dependency) if args.dependency is not None else ""}
{make_gpu_config() if is_gpu else make_cpu_config()}
#SBATCH --exclude={','.join(node_blacklist)}
#SBATCH --container-mounts={','.join([f"{k}:{v}" for k,v in mount_dict.items()])}
#SBATCH --container-image={docker_image_path}

bash {jobdir_path}/command.sh && echo 'done' > {jobdir_path}/job.done
"""


def make_sbatch():
    current_working_dir = Path.cwd().absolute()
    sbatch_path = jobdir_path / f"sbatch.bash"
    docker_image_path = Path(
        "kylevedder_offline_sceneflow_latest.sqsh").absolute()
    assert docker_image_path.is_file(
    ), f"Docker image {docker_image_path} squash file does not exist"

    sbatch_file_content = make_sbatch_file_content(current_working_dir,
                                            docker_image_path, args.num_gpus > 0)

    with open(sbatch_path, "w") as f:
        f.write(sbatch_file_content)


node_blacklist = get_node_blacklist()
make_command_file(args.command)
make_sbatch()
if not args.dry_run:
    print("RUN THIS COMMAND TO SUBMIT THE JOB:")
    print("|")
    print("|")
    print("|")
    print("|")
    print("|")
    print("V")
    print(f"sbatch {jobdir_path}/sbatch.bash")
    print("^")
    print("|")
    print("|")
    print("|")
    print("|")
    print("|")

print(f"Config files written to {jobdir_path.absolute()}")
